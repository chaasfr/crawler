# crawler

[![Go Reference](https://pkg.go.dev/badge/github.com/chaasfr/crawler.svg)](https://pkg.go.dev/github.com/chaasfr/crawler)
[![Go Report Card](https://goreportcard.com/badge/github.com/chaasfr/crawler)](https://goreportcard.com/report/github.com/chaasfr/crawler)
[![Build](https://github.com/chaasfr/crawler/actions/workflows/go.yml/badge.svg)](https://github.com/chaasfr/crawler/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A lightweight and flexible **web crawler in Go** designed for developers and data enthusiasts.  
It fetches web pages, extracts structured information (e.g., headers, links, metadata), and exports reports in CSV format for further analysis.

---

## âœ¨ Features
- Simple CLI interface.
- Extracts and parses HTML elements (headers, links, etc.).
- Generates structured **CSV reports**.
- Written in Go: fast, portable, and easy to extend.

---

## ðŸš€ Getting Started

### Prerequisites
- [Go 1.22+](https://go.dev/dl/)
- Git

### Clone the repository
```bash
git clone https://github.com/chaasfr/crawler.git
cd crawler
