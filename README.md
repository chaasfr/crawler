# crawler

[![Go Reference](https://pkg.go.dev/badge/github.com/chaasfr/crawler.svg)](https://pkg.go.dev/github.com/chaasfr/crawler)
[![Go Report Card](https://goreportcard.com/badge/github.com/chaasfr/crawler)](https://goreportcard.com/report/github.com/chaasfr/crawler)
[![Build](https://github.com/chaasfr/crawler/actions/workflows/go.yml/badge.svg)](https://github.com/chaasfr/crawler/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A lightweight and extensible **web crawler in Go**.  
It visits pages starting from a seed URL, extracts structured content (e.g. headers like `<h1>`), normalizes/filters URLs, and writes a **CSV report** for analysis.

---

## âœ¨ Features

- **Fast & simple CLI** built with Go
- **HTML parsing helpers** for extracting elements (e.g., page headers)
- **URL normalization & filtering** (stay-in-domain, deduping)
- **CSV reporting** for downstream analysis
- **Unit tests** for core components

---

## ğŸ—‚ï¸ Project structure
â”œâ”€â”€ main.go # CLI entrypoint
â”œâ”€â”€ config.go # CLI flags & configuration
â”œâ”€â”€ crawler.go # Crawl orchestration & queueing
â”œâ”€â”€ html.go # HTML parsing utilities (e.g., H1 extraction)
â”œâ”€â”€ url.go # URL normalization & helpers
â”œâ”€â”€ csv_report.go # CSV writer/report schema
â”œâ”€â”€ html_test.go # HTML parsing tests
â””â”€â”€ url_test.go # URL helper tests

## ğŸš€ Quickstart

### Prerequisites
- **Go 1.21+**: https://go.dev/dl/
- **Git**

### Clone & install
```bash
git clone https://github.com/chaasfr/crawler.git
cd crawler
go mod tidy
```

### Run
```bash
go run . {URL} maxConcurrency maxPages
go run . wikipedia.org 5 20 #example
```

### Build and run executable
```bash
go build -o crawler .
./crawler {URL} maxConcurrency maxPages
./crawler wikipedia.org 5 20 #example
```

### Run tests locally
```bash
go test ./...
```